{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5fb7cb2-ffdc-4234-aedd-96d631cbfc9f",
   "metadata": {},
   "source": [
    "## 1. Data preprocessing and export (Maya → JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fd16de-8f48-4b0e-b444-d9005ceb11b1",
   "metadata": {},
   "source": [
    "This project requires exporting the skeleton hierarchy and skinning data from the T-pose file of a character created in Maya + Advanced Skeleton (without facial binding) for subsequent pose reconstruction and scoring/visualization.\n",
    "\n",
    "Run the following script in the Maya Script Editor (Python) to complete the cleanup and export:\n",
    "- `Clean_file_from_AdvRigChar.py`: Clean up the skeleton and mesh (remove controllers, copy weights/rename)\n",
    "- `MayaJointsHierarchyOutput.py`: Export skeleton hierarchy and local binding matrix (including rotation order)至`pose_file_path/maya_default_pose_output`\n",
    "- `MayaGlobalBindingMatrixOutput.py`: Export the global binding matrix (worldMatrix under Bind Pose) of each joint to`pose_file_path/maya_default_pose_output/weight`\n",
    "- `MayaJointListInfluenceOutput.py`: Export the list of influencing joints (influences) for each mesh to`pose_file_path/maya_default_pose_output/weight`\n",
    "- `MayaWeightOutput.py`: Export the vertex weight matrix of each mesh to `pose_file_path/maya_default_pose_output/weight`\n",
    "\n",
    "The above exported products constitute all the inputs for the subsequent **skeleton reconstruction + linear skinning (LBS)**; where `joints_hierarchy_local.json` provides parent/child relationships and **local binding matrices**, `bind_matrices.json` provides **global (world) binding matrices**, and `influences_*` and `weights_*` provide **joint influence order and weights for each vertex**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67da46c5-3169-47b3-9384-c33d59bba13a",
   "metadata": {},
   "source": [
    "## 2. Load T-pose binding data & prepare linear skinning (LBS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853db531-7a12-47c5-8096-7d8cdf142fe6",
   "metadata": {},
   "source": [
    "Here, two core tool modules are called:\n",
    "\n",
    "(A) `T_poseFromMayaProcess.load_local_bind_and_orient`, reads `joints_hierarchy_local.json`, and parses out:\n",
    "- `parent_map`: Parent-child relationships (short names) for each joint.\n",
    "- `sorted_entries`: A list of joints sorted by hierarchical depth (for top-down recursion).\n",
    "- `local_bind`: The **local binding matrix** $L_j \\in \\mathbb{R}^{4\\times 4}$ for each joint, read in column major order.\n",
    "\n",
    "For any joint's **global binding matrix** $G^{\\mathrm{bind}}_j$, it can be calculated recursively:\n",
    "    \n",
    "    $$\n",
    "    G^{\\mathrm{bind}}_j =\n",
    "    \\begin{cases}\n",
    "    L_j, & \\text{if } \\mathrm{parent}(j) = \\varnothing \\\\\n",
    "    G^{\\mathrm{bind}}_{\\mathrm{parent}(j)} \\cdot L_j, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "    $$\n",
    "    \n",
    "    Corresponds to `compute_bind_global(j)` in the code.\n",
    "\n",
    "(B) `process_pose.skin_character`, taken from the exported files in the `.../weight/` directory:\n",
    "- `verts_rest_*.json`: **rest pose vertices** $V_{\\mathrm{rest}}$ for each mesh.\n",
    "- `influences_*.json`: **joint lists and order** for each mesh (aligned with the weight columns).\n",
    "- `weights_*.json`: **per-vertex weights** $W \\in \\mathbb{R}^{V \\times J}$.\n",
    "- `bind_matrices.json`: **Global binding matrices**, which are inverted during loading to obtain $B_j^{-1}$.\n",
    "\n",
    "Given the **joint world matrix** $T_j$ for any pose (here, the binding pose $T_j = G^{\\mathrm{bind}}_j$ is used for verification visualization),  \n",
    "    Perform **linear blend skinning** (LBS) on the vertices:\n",
    "$$\n",
    "    \\tilde{v} =\n",
    "    \\left( \\sum_{j=1}^J w_j \\, T_j \\, B_j^{-1} \\right) v_{\\mathrm{rest}}(h),\n",
    "    \\quad\n",
    "    v = \\Pi(\\tilde{v})\n",
    "    $$\n",
    "    where $v_{\\mathrm{rest}}(h)$ is the homogeneous coordinate, and $\\Pi(\\cdot)$ denotes dehomogenization by dividing by the homogeneous component $w$. Implementation details can be found in `skin_mesh/skin_character`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2d7d8e-4100-4e73-9f16-12a1ff64b1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_file_path = \"/home/s5722875/Notes/Semester3/AIM_MasterProject_PoseOptimize/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d408ecf4-effe-4798-8578-6a3db475c9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import T_poseFromMayaProcess as TP\n",
    "import process_pose as PP\n",
    "\n",
    "Tpose_json_path = pose_file_path + \"maya_default_pose_output/joints_hierarchy_local.json\"\n",
    "parent_map, sorted_entries, local_bind, rotate_order, depth_map = TP.load_local_bind_and_orient(Tpose_json_path)\n",
    "\n",
    "weight_dir = pose_file_path + \"maya_default_pose_output/weight\"\n",
    "Tpose_globals = {j: TP.compute_bind_global(j) for j in local_bind}\n",
    "_, V_concat, per_vertex_weights = PP.skin_character(weight_dir, Tpose_globals, return_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84738f1-2de4-41b6-8cc7-48e9d5f523d2",
   "metadata": {},
   "source": [
    "#### Visualization (Skeleton / Skeleton + Skinned Mesh)\n",
    "\n",
    "`Visualize.plot_skeleton`: Extract translations from each joint world matrix, i.e., joint coordinates, and connect them into a skeleton according to parent_map.\n",
    "\n",
    "`Visualize.plot_skeleton_and_skin`: Overlay the skeleton and skinned point cloud on the same graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1684f1f-8c5b-4a2d-8989-0511da35b52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Visualize\n",
    "Visualize.plot_skeleton(Tpose_globals, parent_map, figsize=(5,5), elev=90, azim=-90)\n",
    "Visualize.plot_skeleton_and_skin(joint_global_matrix=Tpose_globals, parent_map=parent_map,\n",
    "                                     V_concat=V_concat, elev=90, azim=-90, figsize=(5, 5), show_joint_labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0db3ad-0a48-4f20-8fa0-570348993c40",
   "metadata": {},
   "source": [
    "## 3. Pose File Pose and Camera Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81edebb-dad4-4b01-93ac-d3c6fa7df022",
   "metadata": {},
   "source": [
    "First, run the following script in Maya to export the character's pose and camera parameters for the current frame as JSON (can be used for skeletal binding or binding characters with the adv plugin (adv must be cleared first))\n",
    "- `MayaPoseFileDataOutput.py` → `maya_posefile_output/*.json`: Records the **local rotation** (Joint Rotation, JR) and **translation** of each joint.  \n",
    "- `MayaPoseFileCamOutput.py` → `*_cam.json`: Records the camera's **internal parameters** (focal length, principal point, etc.) and **external parameters** (pose).\n",
    "\n",
    "1. Load pose data\n",
    "- Read `pose_JR_orig` (joint local rotation angles), formatted to align with `sorted_entries / local_bind`, for reconstructing the world matrix.\n",
    "\n",
    "2. Generate the world matrix (`pose_globals_orig`, call `process_pose.build_pose_globals`)\n",
    "- Recursively multiply the **local binding matrix** by the **local rotation** to construct the world matrix $G_j$ for each joint starting from the root node $\\mathrm{Root}_M$:  \n",
    "    \n",
    "    $$\n",
    "    G_j =\n",
    "    \\begin{cases}\n",
    "    L^{\\mathrm{bind}}_j \\cdot R_j(\\mathrm{JR}), & \\text{if } \\mathrm{parent}(j) = \\varnothing \\\\\n",
    "    G_{\\mathrm{parent}(j)} \\cdot L^{\\mathrm{bind}}_j \\cdot R_j(\\mathrm{JR}), & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "    $$\n",
    "    \n",
    "    where \\(R_j(\\mathrm{JR})\\) is the Euler rotation matrix based on the rotation order.  \n",
    "\n",
    "3. Load camera parameters (call `process_pose.compute_camera_from_json`)\n",
    "- **Internal parameter matrix** \\(K\\) (pixel units):\n",
    "  \n",
    "    $$\n",
    "    K =\n",
    "    \\begin{bmatrix}\n",
    "    f_x & 0 & c_x \\\\\n",
    "    0 & f_y & c_y \\\\\n",
    "    0 & 0 & 1\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "    \n",
    "    where $f_x, f_y$ are the horizontal/vertical focal lengths, and $c_x, c_y$ are the principal point coordinates.  \n",
    "\n",
    "- **External parameter matrix** $M_{\\mathrm{cam} \\to \\mathrm{world}}$ (4×4): Describes the position and orientation of the camera in the world coordinate system.  \n",
    "\n",
    "\n",
    "**Function**: Converts joint local rotations exported from Maya back into skeleton poses that can be drawn/scored in the world coordinate system, along with camera parameters, for subsequent projection, LoA fitting, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de42f1d-e87e-43ac-9137-da7f3f44a1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import process_pose as PP\n",
    "\n",
    "pose_json_path = pose_file_path + \"maya_posefile_output/G2_stand_bad_front.json\"\n",
    "with open(pose_json_path, 'r') as f:\n",
    "    pose_JR_orig = json.load(f)\n",
    "# Maya pose processed as pose_globals_orig\n",
    "pose_globals_orig = PP.build_pose_globals(sorted_entries, local_bind, parent_map, pose_JR_orig, root_name=\"Root_M\")\n",
    "\n",
    "# pose file cam internal and external parameters\n",
    "cam_json_path = pose_file_path + \"maya_posefile_output/G2_Front_ForBad_cam.json\"\n",
    "K, M_cam_to_world = PP.compute_camera_from_json(cam_json_path, image_width = 960, image_height = 540)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a63c05e-2c0a-45cd-9565-cde86aca2184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import process_pose as PP\n",
    "\n",
    "V_all_dict, V_concat = PP.skin_character(weight_dir, pose_globals_orig)\n",
    "Visualize.plot_skeleton_and_skin(joint_global_matrix=pose_globals_orig,\n",
    "    parent_map=parent_map,V_concat=V_concat,figsize=(5, 5),elev=90, azim=-90,show_joint_labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98690933-a80d-452a-94a5-357e80321639",
   "metadata": {},
   "source": [
    "## 4. LoA (Line of Action) Extraction and Idealization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9677e452-e10f-4fdb-bc5f-8a4180a8d96d",
   "metadata": {},
   "source": [
    "Project 3D poses onto 2D joints, automatically extract LoA point sets, fit **cubic Bézier motion lines**, and obtain more expressive ideal LoA through “**viewpoint correction + control point pushing**”, while returning the fine-tuned `pose_JR` for subsequent optimization.\n",
    "\n",
    "1. Projection to 2D: `project_pose_to_2d`\n",
    "- Transform each joint's world coordinate $X_w$ to the camera coordinate system:  $X_c = R^\\top (X_w - t)$, considering the Maya camera orientation $(+Z → -Z)$ after perspective projection, and multiply by the intrinsic matrix $K$ to obtain the pixel coordinates $(u,v)$. Filter out points behind the camera $(z \\le 0)$.\n",
    "\n",
    "2. LoA candidate extraction: `extract_loa_candidates`\n",
    "- Assemble the trunk sequence (Head → Neck → Chest → Spine → Root) and use the lower trunk to estimate the trend vector \\(T\\).  \n",
    "- Compare the angles between the left and right legs (Hip–Knee, Knee–Ankle) and \\(T\\):\n",
    "- If the angle is less than `align_max_deg` (default 35°), it is considered to be in the same direction as the trunk.\n",
    "- Use `knee_straight_min_deg` (default 135°) to determine if the leg is straight, and decide whether to include the Ankle.  \n",
    "- Output `selected_loa` (including joint names and pixel coordinates).\n",
    "\n",
    "3. Cubic Bézier fitting: `fit_cubic_bezier`\n",
    "- Fix the start and end points as $P_0, P_3$, and obtain $t_i$ by parameterizing the chord length. Establish a least squares equation for $P_1, P_2$:\n",
    "$$\n",
    "B(t) = (1-t)^3 P_0 + 3(1-t)^2 t P_1 + 3(1-t) t^2 P_2 + t^3 P_3, \\quad\\begin{bmatrix}b_1(t_i) & b_2(t_i)\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- Solve for $P_1, P_2$ and the list of $t$. \n",
    "\n",
    "4. Ideal LoA search: `get_ideal_bezier(...)`, (three steps)\n",
    "1. **Original curve straightness check**: Calculate the curvature and determine whether it is C-shaped or S-shaped. If the threshold is met, return directly.  \n",
    "2. **View angle correction**: Perform a yaw angle search on `Root_M` along the world **Y-axis**, reconstructing `pose_globals → 2D → LoA → Bézier` each time, and select the one with the optimal curvature.  \n",
    "3. **Control point pushing**: Push $P_1, P_2$ along the normal direction of $P_0 \\to P_3$ until the curvature meets the criteria or the step size limit is reached. If LoA touches the leg, perform **Root contour level translation** (keep $v$ unchanged, align $u$ to the Bézier contour point), and reverse the displacement from pixels to world coordinates and write it back to `root_pos`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597a0c34-a367-42d3-a2a5-986428aea188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import LoA_optimize as LOAO\n",
    "importlib.reload(LOAO)\n",
    "\n",
    "points_2d = LOAO.project_pose_to_2d(pose_globals_orig, K, M_cam_to_world)\n",
    "_, selected_loa = LOAO.extract_loa_candidates(points_2d, align_max_deg=35, knee_straight_min_deg=135)\n",
    "control_points, _ = LOAO.fit_cubic_bezier(selected_loa)\n",
    "best_ctrl, fixed_p2d, fixed_loa, LoAfix_poseJR = LOAO.get_ideal_bezier(control_points, points_2d, selected_loa, pose_JR_orig,\n",
    "                                                                  sorted_entries, local_bind, parent_map, K, M_cam_to_world,\n",
    "                                                                    push_policy='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c880e0-124d-4be1-a428-0c3ebd22ea07",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAO.visualize_loa_pair(parent_map,\n",
    "                        # before\n",
    "                        points_2d, selected_loa, control_points,\n",
    "                        # after\n",
    "                        fixed_p2d, fixed_loa, best_ctrl,\n",
    "                        titles=('Original', 'Optimized'))\n",
    "pose_globals_LOAO = PP.build_pose_globals(sorted_entries, local_bind, parent_map, LoAfix_poseJR, 'Root_M')\n",
    "points_2d_1 = LOAO.project_pose_to_2d(pose_globals_LOAO, K, M_cam_to_world)\n",
    "LOAO.visualize_loa_pair(parent_map,\n",
    "                        # before\n",
    "                        fixed_p2d, fixed_loa, best_ctrl,\n",
    "                        # after\n",
    "                        points_2d_1, fixed_loa, best_ctrl,\n",
    "                        titles=('Optimized', 'shifted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7c3c3b-9e5a-46dc-a7aa-e45b51687b63",
   "metadata": {},
   "source": [
    "## 5. Scoring Module and Multi-stage Genetic Algorithm Optimization\n",
    "\n",
    "In this stage, the **scoring function** and **joint rotation search space** are defined in stages and optimized sequentially using a genetic algorithm (GA) at each stage, ultimately superimposed to form the globally optimal posture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dbc8be-d1b3-4dd4-9a78-3c74be583da2",
   "metadata": {},
   "source": [
    "### 5.1 One-time preprocessing (role-level)\n",
    "\n",
    "`precompute_segment_r_world_from_weights`: Estimates the maximum radius of each body segment in world coordinates (optional conical scaling) based on the skinning weights and point cloud of the **T-pose**.  \n",
    "- Parameters:  \n",
    "  - **quantile**: Controls the quantile to enhance robustness against outliers.  \n",
    "  - **safety**: Safety factor (amplifies the radius).  \n",
    "  - **weight_thr**: Threshold for filtering joint weights with weak influence.  \n",
    "- Output:\n",
    "- `radii_world`: Geometric reference for subsequent **2D/3D collision detection** and **silhouette** calculation.\n",
    "\n",
    "`detect_hand_contacts_AB`: Detects the contact state between the **hands** and the **torso/arms**.  \n",
    "- Output:\n",
    "  - Anchor set (hand contact anchors) for hand contact penalties in **Stage E**.\n",
    "\n",
    "`detect_grounded_feet_from_orig`: Detects the initial landing foot and ground height `ground_y` based on the **T-pose** position.\n",
    "- Purpose:  \n",
    "  - Used for **ground penetration penalties** in subsequent stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91acf3c0-15a6-45c1-9da2-468af9596262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import score\n",
    "\n",
    "radii_world = score.precompute_segment_r_world_from_weights(Tpose_globals, V_concat, per_vertex_weights,\n",
    "                            quantile=0.80, safety=1.10, weight_thr=0.30, use_cone=True) # T-pose radius\n",
    "hand_anchors = score.detect_hand_contacts_AB(pose_globals_LOAO,radii_world,band_mm=0.3) # Adhere to the determined bandwidth\n",
    "grounded_map, ground_y = score.detect_grounded_feet_from_orig(Tpose_globals,ground_y=None,eps=0.02) # Ground tolerance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bf7419-aa7d-466d-bafd-b2c6266e6437",
   "metadata": {},
   "source": [
    "### 5.2 Joint Rotation Range Definition (by Stage)\n",
    "\n",
    "`OPT_JOINTS`: List of optimizable joints, with each joint corresponding to three rotation axes (X/Y/Z).  \n",
    "\n",
    "`make_ranges`: Specifies the rotation range `(min_deg, max_deg)` for each joint set.  \n",
    "\n",
    "| Stage | Optimizable Joint Range | Primary Purpose |\n",
    "|------|----------------|----------|\n",
    "| **Stage A** | Full body joint mobility (knees rotate only around the Z axis) | Used for **LoA optimization** |\n",
    "| **Stage B** | Only head joints are movable | Optimizes **camera orientation** |\n",
    "| **Stage C** | Arm position recovery (maintains LoA form unchanged) | Fine-tune arm position |\n",
    "| **Stage D** | Leg and lower limb adjustment | Optimize **silhouette + ground contact** |\n",
    "| **Stage E** | Left and right hand and arm refinement | Optimize **silhouette + contact relationships** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c57edf-43b9-41f2-9590-9fb498ad4bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPT_JOINTS = ['Hip_L','Hip_R','RootPart1_M','Spine1_M','Knee_L','Knee_R','Spine1Part1_M','Ankle_L','Ankle_R',\n",
    "              'Chest_M','Neck_M','Shoulder_L','Shoulder_R','NeckPart1_M','Head_M','Elbow_L','Elbow_R','Wrist_L','Wrist_R']\n",
    "\n",
    "Larms_only  = {'Shoulder_L','Wrist_L'}\n",
    "Rarms_only  = {'Shoulder_R','Wrist_R'}\n",
    "legs_only   = {'Hip_L','Hip_R','Ankle_L','Ankle_R'}\n",
    "Z_ONLY_leg  = {'Knee_L','Knee_R'}\n",
    "Z_ONLY_arm  = {'Elbow_L','Elbow_R'}\n",
    "\n",
    "def make_ranges(overrides, default=(0,0)):\n",
    "    \"\"\"overrides: list of (joint_set, [(x1,x2),(y1,y2),(z1,z2)])\"\"\"\n",
    "    pr = []\n",
    "    for jn in OPT_JOINTS:\n",
    "        for js, rng in overrides:\n",
    "            if jn in js:\n",
    "                pr += rng\n",
    "                break\n",
    "        else:\n",
    "            pr += [default, default, default]\n",
    "    return pr\n",
    "\n",
    "stageA_param_ranges = lambda: make_ranges([\n",
    "    (Larms_only | Rarms_only | Z_ONLY_arm, [(0,0),(0,0),(0,0)]),\n",
    "    (Z_ONLY_leg,  [(0,0),(0,0),(-10,10)]),\n",
    "    (set(OPT_JOINTS), [(-30,30),(-30,30),(-30,30)])\n",
    "])\n",
    "\n",
    "stageB_param_ranges = lambda: make_ranges([\n",
    "    ({'Head_M'}, [(-45,45),(-45,45),(-45,45)])\n",
    "])\n",
    "\n",
    "stageC_param_ranges = lambda: make_ranges([\n",
    "    (Larms_only | Rarms_only, [(-45,45),(-45,45),(-45,45)]),\n",
    "    (Z_ONLY_arm, [(0,0),(0,0),(-45,45)])\n",
    "])\n",
    "\n",
    "stageD_param_ranges = lambda: make_ranges([\n",
    "    (Z_ONLY_leg, [(0,0),(0,0),(-15,90)]),\n",
    "    (legs_only,  [(-90,90),(-90,90),(-90,90)])\n",
    "])\n",
    "\n",
    "stageE_param_ranges = stageC_param_ranges  # E and C are the same\n",
    "\n",
    "joint_idx_map = {jn: i*3 for i, jn in enumerate(OPT_JOINTS)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781af460-7ed3-4663-b1fa-04205e2ea4a7",
   "metadata": {},
   "source": [
    "### 5.3 Scoring function `evaluate_pose`\n",
    "\n",
    "The input is a rotation increment vector (length = `len(OPT_JOINTS) × 3`), and the fitness `fitness` is output after combining the reference pose by stage:\n",
    "\n",
    "1. Apply rotation increments\n",
    "- Overlay the XYZ Euler angle increments of each joint onto `LoAfix_poseJR` to construct a new `pose_globals`.\n",
    "\n",
    "2. Calculation methods and meanings of various scoring/penalty functions called in `score.py`\n",
    "\n",
    "- `line_of_action_score(selected_loa, best_ctrl)`: Make the LoA joint point sequence more closely fit the fitted cubic Bézier curve. Method: Sample the Bézier curve, calculate the mean square distance from the LoA point to the curve, and map it to (0,1].\n",
    "  - Sample curve point $B(t)$, take the square of the distance $d_i^2$ between each LoA point $p_i$ and the nearest sampled point;\n",
    "$$S_{\\text{fit}}=\\frac{1}{1+\\lambda_{\\text{fit}}\\,E_{\\text{fit}}}\\in(0,1]$$\n",
    "  - Return $(S_{\\text{fit}}, E_{\\text{fit}}, \\sqrt{d_i^2})$. When `agg_roothip=True`, the left and right hips are aggregated with the root for noise reduction and stability.\n",
    "\n",
    "- `camera_facing_score(pose_globals, M_cam_to_world, joint_name=\"Head_M\")`: Makes the specified joint (default head) face the camera.\n",
    "  - Take the world pose $T_{\\text{head}}$ of the head and the world pose $T_{\\text{cam}}$ of the camera, and calculate the cosine of the angle between the unit vector $\\hat{v}{\\to \\mathrm{cam}}$ (from the head to the camera) and the head orientation vector $\\hat{v}{\\text{head}}$:\n",
    "$$S_{\\text{cam}}=\\mathrm{clamp}_{[0,1]}(\\cos\\theta).$$\n",
    "The score is $\\cos\\theta$ clamped to $[0,1]$, with 1 for fully facing ( $\\theta=0^\\circ$ ) and 0 for facing away ( $\\theta\\ge 90^\\circ$ ).\n",
    "\n",
    "- Rigid alignment penalty `rigid_penalty_global_v2`: Constrains the positions/orientations of several key joints relative to the reference pose (original or previous stage result) from deviating too much.\n",
    "  - Single joint cost:\n",
    "$$w_t\\,\\|\\mathbf{t}_{\\text{now}}-\\mathbf{t}_{\\text{ref}}\\|_2 + w_r\\,\\angle(R_{\\text{now}},R_{\\text{ref}}),$$\n",
    "where the rotation error is the geometric angle $\\angle(\\cdot)$; the sum of multi-joint values is returned with a **negative sign** as the penalty value (the more negative, the worse). Optionally, apply a light anchor point to stabilize the foot.\n",
    "\n",
    "- Runtime radius and 2D capsule `capsule_r_px_runtime` / `build_capsules_px`: Convert the world radius of body segments obtained by weighting statistics in T-pose to pixel radius, and construct a 2D capsule of “line segment + radius” on the image plane to evaluate the silhouette distance.\n",
    "  - World radius $;r_{\\text{world}}\\to$ pixel radius:\n",
    "$$r_{px} = z_{\\mathrm{cam}} \\, f_x \\, r_{\\mathrm{world}}$$\n",
    "The cone radius is taken as the larger value of the two end projections as a precaution. Then, assemble the 2D capsule using the endpoints $(u,v)$ and $r_{\\text{px}}$.\n",
    "\n",
    "- Silhouette term `silhouette_score`: Maintain a clear gap between key limbs and the torso in 2D, and try to maintain the LoA direction (`keep_ctrl`).\n",
    "  - Minimum separation: Calculate the “minimum boundary distance” between two capsules (or point–capsules): $\\mathrm{minsep} = d_{\\mathrm{seg/pt}} - (r_A + r_B)$\n",
    "- Map the minimum distance to a reward in the range $[-\\text{overlap},1]$:\n",
    "$$\\begin{cases} \\mathrm{minsep}/\\text{target}, & \\mathrm{minsep}\\ge 0\\\\[2pt] \\max(-\\text{overlap},\\,\\mathrm{minsep}/(-\\text{target})), & \\mathrm{minsep}<0 \\end{cases}$$\n",
    "- Combination term (weighted by weight):\n",
    "    - Torso–leg (only count the side without LoA, prioritize moving the leg that does not carry LoA)\n",
    "- Torso–arm, leg–leg, arm–arm, arm–leg\n",
    "    - LoA keep term: perform `line_of_action_score` again with `keep_ctrl` and the current LoA\n",
    "$$w_{\\text{leg\\_torso}}\\,\\mathrm{reward}+\\cdots+w_{\\text{keep}}\\,S_{\\text{fit}}^{(\\text{keep})}.$$\n",
    "Weights and target seam width (pixels) are configurable; smaller for distant scenes, larger for close-up scenes.\n",
    "\n",
    "- 3D self-collision penalty `collision_penalty_capsule3d`: Approximate capsules using line segments + radius in the 3D world to suppress key parts from intersecting.\n",
    "  - Calculate the shortest 3D line segment distance $d_{\\text{3D}}$ for a pair of segments $(A,B)$, and compare it with the capsule radii: $\\mathrm{sep}_{3\\mathrm{D}} = d_{3\\mathrm{D}} - (r_A + r_B)$\n",
    "  - If $\\mathrm{sep}_{3\\text{D}}<0$, apply a positive penalty and normalize by `tgt_mm`; sum over the top-k worst cases to suppress noise. Higher values indicate worse performance.\n",
    "\n",
    "- Ground penalty `ground_penalty`: For feet marked as “should be on the ground,” avoid passing through the ground and maintain a reasonable foot orientation.\n",
    "  - Ground penetration: If the toe/sole $y<y_{\\text{ground}}$, deduct points linearly based on depth;  \n",
    "  - Orientation: If $\\angle(\\overrightarrow{\\text{Ankle}\\to\\text{Toe}}, ,+Y)$ exceeds the threshold (default $25^\\circ$), deduct points.\n",
    "\n",
    "- Hand contact penalty/maintenance `detect_hand_contacts_AB` & `hand_contact_penalty`: Automatically detect hand contact anchor points from the previous A+B phase (e.g., wrist touching the torso/thigh/calf), and encourage “contact with the surface or slight separation” in the E phase to avoid penetration or excessive separation.\n",
    "  - Detection: Calculate the separation between the wrist and the target segment in the AB pose as $\\mathrm{sep}_{3\\text{D}}=d-r(\\cdot)$. If it falls within $[-b,+b]$, it is considered a contact anchor point;\n",
    "  - Stage E penalty: For anchor points, the expected range is $0\\le \\mathrm{sep}_{3\\text{D}}\\le b$. Outside this range, penalties are applied linearly based on the deviation amount/bandwidth (heavy penalties for overlapping, light penalties for being too far away `soft`).\n",
    "\n",
    "3.Stage scoring parameters\n",
    "\n",
    "- Stage A (LoA) $f = \\mathrm{LoA\\_score}(\\text{selected\\_loa}, \\text{best\\_ctrl})$\n",
    "- Stage B (camera orientation + rigid constraints) $f = 1.0 \\times S_{\\mathrm{cam\\_head}} + 0.2 \\times P_{\\mathrm{rigid}}$\n",
    "- Stage C (Hand Position Constraint) $f = P_{\\mathrm{rigid}}(\\mathrm{Wrist\\_L}, \\mathrm{Wrist\\_R})$\n",
    "- Stage D (silhouette + collision + ground + rigid constraints) $f = 1.0 \\times S_{\\mathrm{sil}} - 0.25 \\times P_{\\mathrm{coll3d}} - 0.40 \\times P_{\\mathrm{ground}} + 0.10 \\times P_{\\mathrm{rigid}}$\n",
    "- Stage E (silhouette + collision + hand contact) $f = 1.0 \\times S_{\\mathrm{sil}} - 0.20 \\times P_{\\mathrm{coll3d}} - 0.60 \\times P_{\\mathrm{hand\\_contact}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52844c8-85f4-428f-bf14-2af7ec9da897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import score\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "def ranges_to_mask(param_ranges):\n",
    "    # 0/1 mask: Set positions in the range (0,0) to 0.\n",
    "    m = []\n",
    "    for lo, hi in param_ranges:\n",
    "        m.append(0.0 if (lo == 0 and hi == 0) else 1.0)\n",
    "    return np.array(m, dtype=float)\n",
    "\n",
    "def evaluate_pose(vector, stage='A', joint_order=OPT_JOINTS, base_vec=None, base_mask=None, param_ranges=None):  \n",
    "\n",
    "    # —— Combination vector: base_vec + mask * vector ——\n",
    "    vec = np.array(vector, dtype=float)\n",
    "    if base_vec is None:\n",
    "        composed = vec\n",
    "    else:\n",
    "        if base_mask is None:\n",
    "            base_mask = np.ones_like(vec)\n",
    "        composed = np.array(base_vec, float) + np.array(base_mask, float) * vec\n",
    "        \n",
    "    # 1) Deep copy posture, with optional root translation\n",
    "    pose_JR = copy.deepcopy(LoAfix_poseJR)\n",
    "\n",
    "    # 2) Superimpose the rotation increments of each joint\n",
    "    for jn in joint_order:\n",
    "        idx = joint_idx_map[jn]\n",
    "        dx, dy, dz = composed[idx:idx+3]\n",
    "        ox, oy, oz = pose_JR[\"joint_rot\"][jn]\n",
    "        pose_JR[\"joint_rot\"][jn] = [ox+dx, oy+dy, oz+dz]\n",
    "\n",
    "    # 3) Build global & loa preparation\n",
    "    pose_globals = PP.build_pose_globals(sorted_entries, local_bind, parent_map, pose_JR, 'Root_M')\n",
    "    points_2d    = LOAO.project_pose_to_2d(pose_globals, K, M_cam_to_world) #计算loa评分\n",
    "    _, selected_loa = LOAO.extract_loa_candidates(points_2d, align_max_deg=35, knee_straight_min_deg=135)\n",
    "\n",
    "    # 4) Stage-specific scoring\n",
    "    if stage == 'A':\n",
    "        loa_sc, _, _ = score.line_of_action_score(selected_loa, best_ctrl)  # LoA\n",
    "        fitness = (1.0 * loa_sc)\n",
    "        return (float(fitness),)\n",
    "\n",
    "    elif stage == 'B':\n",
    "        cam_head = score.camera_facing_score(pose_globals, M_cam_to_world, joint_name=\"Head_M\")\n",
    "        const_head = score.rigid_penalty_global_v2(pose_globals, pose_globals_orig,\n",
    "                                        constraints={'HeadEnd_M':  dict(w_t=0.005, w_r=0),},use_toe_anchor=True)   # 位置旋转约束\n",
    "        fitness = (1.0 * cam_head + 0.2 * const_head)\n",
    "        return (float(fitness),)\n",
    "\n",
    "    elif stage == 'C':\n",
    "        const = score.rigid_penalty_global_v2(pose_globals, pose_globals_LOAO, constraints={\n",
    "                                            'Wrist_L': dict(w_t=1, w_r=0.2),'Wrist_R': dict(w_t=1, w_r=0.2),},\n",
    "                                             use_toe_anchor=False)   # 位置旋转约束\n",
    "        fitness = (1.0 * const)\n",
    "        return (float(fitness),)\n",
    "\n",
    "    elif stage == 'D':\n",
    "        r_px_by_seg = score.capsule_r_px_runtime(pose_globals, K, M_cam_to_world, radii_world) # 运行时半径\n",
    "        sc_sil, sil_terms, sil_argmins, move_leg = score.silhouette_score(points_2d, r_px_by_seg,\n",
    "                                                    keep_ctrl=best_ctrl, selected_loa=selected_loa,\n",
    "                                                    target_gap_px=10, w_leg_torso=0.7, w_elbow_torso=0.0, w_keep=0.3,\n",
    "                                                     w_arm_torso=0.0, w_leg_leg=0.15, w_arm_arm=0.0, w_arm_leg=0.0)\n",
    "        coll3d = score.collision_penalty_capsule3d(pose_globals, radii_world, topk=4, tgt_mm=0.1)\n",
    "        gpen   = score.ground_penalty(pose_globals, grounded_map, ground_y, up_vec_thresh_deg=25.0)\n",
    "        const = score.rigid_penalty_global_v2(pose_globals, pose_globals_LOAO, constraints={\n",
    "                                            'Ankle_L': dict(w_t=0, w_r=0.2),'Ankle_R': dict(w_t=0, w_r=0.2),},\n",
    "                                            use_toe_anchor=False)   # 位置旋转约束\n",
    "        fitness = (1.00 * sc_sil - 0.25 * coll3d - 0.40 * gpen + 0.10 * const)\n",
    "        return (float(fitness),)\n",
    "\n",
    "    elif stage == 'E':\n",
    "        r_px_by_seg = score.capsule_r_px_runtime(pose_globals, K, M_cam_to_world, radii_world) # 运行时半径\n",
    "        sc_sil, sil_terms, sil_argmins, move_leg = score.silhouette_score(points_2d, r_px_by_seg,\n",
    "                                                    keep_ctrl=best_ctrl, selected_loa=selected_loa,\n",
    "                                                    target_gap_px=5, w_leg_torso=0.0, w_elbow_torso=0.0, w_keep=0.0,\n",
    "                                                     w_arm_torso=0.2, w_leg_leg=0.0, w_arm_arm=0.2, w_arm_leg=0.15)\n",
    "        coll3d = score.collision_penalty_capsule3d(pose_globals, radii_world, topk=4, tgt_mm=0.1)\n",
    "        hpen = score.hand_contact_penalty(pose_globals, radii_world, hand_anchors, band_mm=0.3, soft=1.0)\n",
    "        fitness = (1.00 * sc_sil - 0.20 * coll3d - 0.60 * hpen )\n",
    "        return (float(fitness),)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4606fa6-a171-48f7-ac78-4122dfa361e2",
   "metadata": {},
   "source": [
    "### 5.4 Multi-stage GA optimization process\n",
    "- Optimizer: `Model.optimize_pose_GA` (based on the DEAP framework)\n",
    "- Population size: `mu` / `lambda_`, Gaussian mutation: `mutGaussian(sigma)`, Crossover: `cxBlend`\n",
    "  - Supports stage-based **parameter freezing mask** `mask` and seed vector `seed_vec`\n",
    "\n",
    "\n",
    "- Execution order\n",
    "$$base\\_ABCDE = A + B + C + D + E$$\n",
    "\n",
    "Avoid slow convergence and conflicts caused by a single global search, ensuring the optimization sequence aligns with animators' work habits: LoA → Camera → Arms → Legs → Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab7831d-0583-460b-82ca-eafed126d4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Model\n",
    "importlib.reload(Model)\n",
    "\n",
    "# —— Stage A loa ——\n",
    "prA = stageA_param_ranges()\n",
    "fnA = lambda v: evaluate_pose(v, stage='A', param_ranges=prA)\n",
    "bestA = Model.optimize_pose_GA(fnA, prA, ngen=20, mu=30, lambda_=60, sigma=5.0, seed_vec=None)\n",
    "\n",
    "# —— Stage B Cam Facing ——\n",
    "prB = stageB_param_ranges()\n",
    "maskB = ranges_to_mask(prB)\n",
    "fnB = lambda v: evaluate_pose(v, stage='B', base_vec=bestA, base_mask=maskB, param_ranges=prB)\n",
    "bestB = Model.optimize_pose_GA(fnB, prB, ngen=20, mu=30, lambda_=60, mutpb=0.4, sigma=5.0, seed_vec=np.zeros(len(prB)), seed_spread=1.0)\n",
    "\n",
    "# —— Stage C adjust arm ——\n",
    "base_AB = np.array(bestA) + maskB * np.array(bestB)\n",
    "prC = stageC_param_ranges()\n",
    "maskC = ranges_to_mask(prC)\n",
    "fnC = lambda v: evaluate_pose(v, stage='C', base_vec=base_AB, base_mask=maskC, param_ranges=prC)\n",
    "bestC = Model.optimize_pose_GA(fnC, prC, ngen=30, mu=45, lambda_=80, sigma=5.0, mutpb=0.4,seed_vec=np.zeros(len(prC)), seed_spread=1.5)\n",
    "\n",
    "# —— Stage D leg silhouette ——\n",
    "base_ABC = np.array(base_AB) + maskC * np.array(bestC)\n",
    "prD = stageD_param_ranges()\n",
    "maskD = ranges_to_mask(prD)\n",
    "fnD = lambda v: evaluate_pose(v, stage='D', base_vec=base_ABC, base_mask=maskD, param_ranges=prD)\n",
    "bestD = Model.optimize_pose_GA(fnD, prD, ngen=20, mu=30, lambda_=60, mutpb=0.4, sigma=4.0, seed_vec=np.zeros(len(prD)), seed_spread=0.8)\n",
    "\n",
    "# —— Stage E hand silhouette ——\n",
    "base_ABCD = np.array(base_ABC) + maskD * np.array(bestD)\n",
    "prE = stageE_param_ranges()\n",
    "maskE = ranges_to_mask(prE)\n",
    "fnE = lambda v: evaluate_pose(v, stage='E', base_vec=base_ABCD, base_mask=maskE, param_ranges=prE)\n",
    "bestE = Model.optimize_pose_GA(fnE, prE, ngen=20, mu=30, lambda_=60, sigma=3.0, seed_vec=np.zeros(len(prE)), seed_spread=0.8)\n",
    "\n",
    "base_ABCDE = np.array(base_ABCD) + maskE * np.array(bestE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbf74cd-41d9-46eb-bebd-88d62846fb88",
   "metadata": {},
   "source": [
    "## 6. Apply optimization results and send back to Maya\n",
    "- `apply_offset`: Overlay the optimized vectors on the reference pose `LoAfix_poseJR` joint by joint to generate the refined pose `pose_JR_refined`, which can be saved as JSON (for use in Maya).\n",
    "\n",
    "- **Visualization**: Use `build_pose_globals` to reconstruct the global matrix, then use `plot_skeleton` to plot it; this can be called after each stage to view the results.\n",
    "\n",
    "- **Back to Maya**: Run `RefinedPoseBackToMaya.py` in Maya, read the JSON, and set the root node pose and joint `.rotateX/Y/Z` roles to switch to the optimized pose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f1b17a-f460-4037-a552-d32929d3b66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Visualize\n",
    "\n",
    "pose_JR_refined = PP.apply_offset(base_ABC, LoAfix_poseJR, OPT_JOINTS, joint_idx_map, outfile= pose_file_path+\"refined_pose.json\")\n",
    "\n",
    "# 2) Re-run build_pose_globals on that refined dict\n",
    "refined_pose_globals = PP.build_pose_globals(sorted_entries,local_bind,parent_map,pose_JR_refined,root_name=\"Root_M\")\n",
    "# 3) Visualize\n",
    "Visualize.plot_skeleton(refined_pose_globals, parent_map,figsize=(4,4), elev=90, azim=-90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe3393a-44f0-40d7-bdec-cde127f2dc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Visualize\n",
    "\n",
    "pose_JR_refined = PP.apply_offset(base_ABCD, LoAfix_poseJR, OPT_JOINTS, joint_idx_map, outfile= pose_file_path+\"refined_pose.json\")\n",
    "\n",
    "# 2) Re-run build_pose_globals on that refined dict\n",
    "refined_pose_globals = PP.build_pose_globals(sorted_entries,local_bind,parent_map,pose_JR_refined,root_name=\"Root_M\")\n",
    "# 3) Visualize\n",
    "Visualize.plot_skeleton(refined_pose_globals, parent_map,figsize=(4,4), elev=90, azim=-90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b930b7-2c42-452a-8387-d9422214abdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Visualize\n",
    "\n",
    "pose_JR_refined = PP.apply_offset(base_ABCDE, LoAfix_poseJR, OPT_JOINTS, joint_idx_map, outfile= pose_file_path+\"refined_pose.json\")\n",
    "\n",
    "# 2) Re-run build_pose_globals on that refined dict\n",
    "refined_pose_globals = PP.build_pose_globals(sorted_entries,local_bind,parent_map,pose_JR_refined,root_name=\"Root_M\")\n",
    "# 3) Visualize\n",
    "Visualize.plot_skeleton(refined_pose_globals, parent_map,figsize=(4,4), elev=90, azim=-90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f1e887-40fc-4c1e-b1cc-30e2cf7cdbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import process_pose as PP\n",
    "importlib.reload(PP)\n",
    "\n",
    "V_all_dict, V_concat = PP.skin_character(weight_dir, refined_pose_globals)\n",
    "Visualize.plot_skeleton_and_skin(joint_global_matrix=refined_pose_globals,\n",
    "    parent_map=parent_map,V_concat=V_concat,figsize=(5, 5),elev=90, azim=-90,show_joint_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17120cdb-e9be-4add-9871-20e3729b2d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907b0fd1-acc7-4735-96ac-f34dd55215dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d5b73a-fedd-47e4-8803-04028696c3b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
